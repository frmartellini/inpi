{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "109ljONzHOP_"
   },
   "outputs": [],
   "source": [
    "## Load necessary libraries\n",
    "\n",
    "import requests # module to send HTTP requests\n",
    "import os # module that provide a portable way of using operating system dependent functionality\n",
    "import pandas as pd # data analysis and data manipulation tools\n",
    "import numpy as np # fundamental package for scientific computing\n",
    "import sys # system-specific and parameters and functions\n",
    "import zipfile # work with zip archives\n",
    "import io # core tools to work with data streams\n",
    "from bs4 import BeautifulSoup # library to scrape information from HTML or XML files\n",
    "import re # regular expression operations\n",
    "\n",
    "print(\"All necessary libraries are loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Overriding the default warning filter to hide ssl warnings from users\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions and classes to parse, download and generate reports from the\n",
    "## Intelectual Property Journal (RPI - Revista de Propriedade Intelectual)\n",
    "\n",
    "# Class to access and extract information from patent at INPI web application\n",
    "class api_inpi:\n",
    "    # Constructor to initialize the properties of the class\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "    \n",
    "    # Return the status of the files on the server\n",
    "    #     Use: api_inpi(url).url_status() or\n",
    "    #          self.url_status() inside class functions\n",
    "    def url_status(self):\n",
    "        r = requests.head(self.url) # make a HEAD request to the web page and return the HTTP header\n",
    "        return r.status_code\n",
    "    \n",
    "    # Extract the xml file from the zip package os the patentes section at RPI\n",
    "    #     Use: api_inpi(url).xml_extract() or\n",
    "    #          self.xml_extract() inside class functions\n",
    "    def xml_extract(self):\n",
    "        if self.url_status() == 200: # if the returned status code is 200, proceeds with the download\n",
    "            r = requests.get(url, stream=True) # request data from the resource\n",
    "            z = zipfile.ZipFile(io.BytesIO(r.content)) # download a ZIP file and extract its contents in memory\n",
    "            f_names = z.namelist() # list the filenames inside the ZIP package in f_names\n",
    "            for fileName in f_names: # for every file in list of names\n",
    "                if fileName.endswith('.xml'): # check if the filename ends with a xml file extension\n",
    "                    z.extract(fileName) # and extract the single file xml file from zip\n",
    "            return fileName # return the name of the xml file\n",
    "        \n",
    "        if self.url_status() == 302: # if the returned status code is 302, the page was temporarily moved or the file does not yet exist\n",
    "            print(\"The requested file does not exist.\")\n",
    "            \n",
    "    # Search the patents documents by holder at RPI\n",
    "    #     Use: api_inpi(url).busca_titular(rpi number,\"holder name\",dataframe name) or\n",
    "    #          self.api_inpi(url).busca_titular(rpi number,\"holder name\",dataframe name) inside class functions\n",
    "    def busca_titular(self,revista,org,df):\n",
    "        rpi = revista # RPI number\n",
    "        f_xml = self.xml_extract() # retrieve the xml file for the RPI\n",
    "        infile = open(f_xml,'r') # open the xml downloaded file\n",
    "        xml_data = infile.read() # and store the content in a variable\n",
    "        xml_data = re.sub(' +',' ',xml_data) # replaces multiple spaces with single spaces in xml file\n",
    "        soup = BeautifulSoup(xml_data, 'xml') # passes the stored data to the beautifulsoup analyzer and stores the returned object\n",
    "        org_soup = soup.find_all('nome-completo', text=lambda x: x is not None and org in x.casefold()) # search how many times the name of the holder appeared in the xml file\n",
    "        data_revista = soup.find('revista').attrs.get('dataPublicacao') # extract the publication date for the given RPI\n",
    "        despacho_len = len(org_soup) # counts the total amount of records found\n",
    "        \n",
    "        for i in range(despacho_len): # browse the file to extract the information where the holder is founded\n",
    "            despacho_soup = org_soup[i].find_parent('despacho') # for i-th element, find the parent tag <despacho> for the holder           \n",
    "            registro_soup = soup.find_all('numero', text=lambda x: despacho_soup.find('numero').text in x) # for i-th element, search how many times the protection number was founded\n",
    "            registro_len = len(registro_soup) # count the number of times that the protection number is repeated\n",
    "            gestao = \"\" # set the variable to identify if the organization is the owner or co-owner of the patent\n",
    "            flag = 1 # set flag to 1 to indicate that is necessary to discover if organization is the patent owner\n",
    "            \n",
    "            # protection number can happen more than one time to a given RPI , so we need to search the information\n",
    "            # to all records with the same number\n",
    "            for j in range(registro_len): # for j-th element, extract the selected information based on the protection number\n",
    "                registros = registro_soup[j].find_parent('despacho')\n",
    "        \n",
    "                cod_despacho = registros.find('codigo') # search for the dispatch code\n",
    "                if cod_despacho != None: # if the attribute is found\n",
    "                    cod_despacho = cod_despacho.text # store the content in a variable\n",
    "                    \n",
    "                titulo = registros.find('titulo', inid=\"54\") # search for the title\n",
    "                if titulo != None: # if the attribute is found\n",
    "                    titulo = titulo.text # store the content in a variable\n",
    "        \n",
    "                registro = registros.find('numero') # search for the protection number\n",
    "                if registro != None: # if the attribute is found\n",
    "                    registro = registro.text # store the content in a variable\n",
    "                    \n",
    "                # set the default codes to make the search for the documents at web server\n",
    "                # if the protection number starts with BR, remove BR, the spaces and the last two digits\n",
    "                # else, remove only the spaces and the last two digits\n",
    "                cod_busca = registro[:-2].replace(' ', '').replace('BR', '')\n",
    "                                \n",
    "                kindcode = registros.find('numero').attrs.get('kindcode') # search and store the protection kind code\n",
    "                    \n",
    "                data_deposito = registros.find('data-deposito') # search for the deposit date\n",
    "                if data_deposito != None: # if the attribute is found\n",
    "                    data_deposito = data_deposito.text # store the content in a variable\n",
    "                \n",
    "                ci_len = len(registros.find_all('classificacao-internacional')) # search for the international classification info\n",
    "                ci = \"\" # set the variable to store the information\n",
    "                if ci_len > 0: # if the information is found\n",
    "                    for k in range(ci_len): # for the k-th element\n",
    "                        # store internation classification information in the format\n",
    "                        # internation classification code (international classification year) | ...\n",
    "                        ci_ano = registros.find_all('classificacao-internacional')[k].text + \" (\" + registros.find_all('classificacao-internacional')[k].attrs.get('ano') + \")\"\n",
    "                        ci = ci + ci_ano + \" | \"\n",
    "                ci = ci[:-3] # delete the last three digits\n",
    "            \n",
    "                titular_1 = registros.find('titular', sequencia=\"1\") # check if the organization is the patent owner\n",
    "                if titular_1 != None and flag == 1: # if there is a first holder and flag is equal 1\n",
    "                    nome_completo = titular_1.find('nome-completo').text # store the first holder name \n",
    "                    if org.lower() not in nome_completo.lower(): # if the first holder is not the organization, define the management variable as No\n",
    "                        gestao = \"Não\"\n",
    "                    if org.lower() in nome_completo.lower(): # if the first holder is the organization, define the management variable as Yes\n",
    "                        gestao = \"Sim\"\n",
    "                flag = 0 # set flag as 0, to indicate that is not necessary define the patent ownership if the protection code repeat\n",
    "\n",
    "                titular_len = len(registros.find_all('titular')) # search for the ownership list\n",
    "                titulares = \"\" # set the variable to store the information\n",
    "                if titular_len > 0: # if the information is found\n",
    "                    for k in range(titular_len): # for the k-th element\n",
    "                        # store ownership information in the format\n",
    "                        # organizartion name | ...\n",
    "                        titular = registros.find_all('titular')[k].find('nome-completo').text\n",
    "                        titulares = titulares + titular + \" | \"\n",
    "                titulares = titulares[:-3] # delete the last three digits\n",
    "    \n",
    "                inventor_len = len(registros.find_all('inventor')) # seacrh for the list of inventors\n",
    "                inventores = \"\" # set the variable to store the information\n",
    "                if inventor_len > 0: # if the information is found\n",
    "                    for k in range(inventor_len): # for the k-th element\n",
    "                        # store inventors name in the format\n",
    "                        # inventor name | ...\n",
    "                        inventor = registros.find_all('inventor')[k].find('nome-completo').text\n",
    "                        inventores = inventores + inventor + \" | \"\n",
    "                inventores = inventores[:-3] # delete the last three digits\n",
    "                \n",
    "                # store the information extracted in the i-th line of the dataframe\n",
    "                df.loc[len(df)] = [rpi, data_revista,registro, kindcode, cod_despacho,titulo,gestao, titulares, inventores, data_deposito, ci,cod_busca]\n",
    "        \n",
    "        df['rpi'] = df['rpi'].astype('int') # set the rpi column at dataframe as a integer type to avoid conflicts\n",
    "        df['cod_busca'] = df['cod_busca'].astype('str') # set the cod_busca column as a string type to avoid conflicts\n",
    "        df['cod_despacho'] = df['cod_despacho'].astype('str') # set the cod_despacho column as a string type to avoid conflicts\n",
    "        os.remove(f_xml) # delete the xml file\n",
    "\n",
    "# Generate a report in xlsx format with the information parsed from the RPI\n",
    "#     Use: excel_report(source dataframe,file name,inside file sheet name)\n",
    "def excel_report(xls_df,xls_name,sheet_name):\n",
    "    writer = pd.ExcelWriter(xls_name + \".xlsx\", engine='xlsxwriter', date_format='dd/mm/yyyy') # create a pandas excel writer using XlsxWriter\n",
    "    xls_df.to_excel(writer, sheet_name=sheet_name, index=False) # convert the dataframe to an XlsxWriter Excel object\n",
    "    workbook  = writer.book # get the xlsxwriter workbook object\n",
    "    worksheet = writer.sheets[sheet_name] # get the xlsxwriter worksheet object\n",
    "    \n",
    "    for i, col in enumerate(xls_df.columns): # iterate through each column and set the width to the max length\n",
    "        column_len = xls_df[col].astype(str).str.len().max() # find length of column i\n",
    "        column_len = max(column_len, len(col)) + 2 # setting the length if the column header is larger than the max column value length\n",
    "        worksheet.set_column(i, i, column_len) # set the column length\n",
    "        \n",
    "    writer.save() # close the Pandas Excel writer and output the Excel file\n",
    "    print(\"RPI Report concluded!\")\n",
    "\n",
    "print(\"Bibliotecas carregadas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RPI selection\n",
    "\n",
    "rpi_i =  input(\"Digite o número da primeira RPI:\\n\")\n",
    "rpi_i = int(rpi_i)\n",
    "rpi_f =  input(\"Digite o número da última RPI:\\n(Caso seja somente uma edição, aperte ENTER)\\n\") or rpi_i\n",
    "rpi_f = int(rpi_f)\n",
    "\n",
    "if rpi_f > rpi_i: # if the last rpi is greather than firts\n",
    "    print(\"The information will be parsed from the RPI\", rpi_i,\"to the\", rpi_f)\n",
    "if rpi_f <= rpi_i: # else\n",
    "    print(\"The information will be parsed from the RPI\", rpi_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make the search in patent section of the RPI\n",
    "\n",
    "# Set the columns and create the dataframe to store information\n",
    "column_names = ['rpi','data de publicação','registro','kind code','cod_despacho','título','gestão','titulares','inventores','data de deposito','classificação internacional','cod_busca'] # cria as colunas do dataframe\n",
    "patentes_df = pd.DataFrame(columns=column_names)\n",
    "\n",
    "rpi = rpi_i # set rpi counter as rpi_i\n",
    "for i in range(rpi_f-rpi_i+1): # for the i-th edition from the rpi\n",
    "    url = \"http://revistas.inpi.gov.br/txt/P\" + str(rpi) + \".zip\" # set the url to make the search\n",
    "    api_inpi(url).busca_titular(rpi,'universidade estadual de campinas',patentes_df) # make the search\n",
    "    rpi+=1 # ident the rpi counter\n",
    "\n",
    "if rpi_f > rpi_i: # if the last rpi is greather than firts\n",
    "    f_name = \"rpi_patentes_\" + str(rpi_i) + \"-\" + str(rpi_f) # set the xlsx filename\n",
    "    s_name = str(rpi_i) + \" - \" + str(rpi_f) # set the sheet filename\n",
    "if rpi_f <= rpi_i: # else \n",
    "    f_name = \"rpi_patentes_\" + str(rpi_i)  # set the xlsx filename\n",
    "    s_name = str(rpi_i)  # set the sheet filename\n",
    "\n",
    "excel_report(patentes_df.iloc[:,:-1],f_name,s_name) # generate the xlsx report with extracted information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download the files from the web server\n",
    "\n",
    "# creates the columns with the file names and the folder names to download\n",
    "# in this example we use this pattern to folders, with three levels\n",
    "#     PareceresRPI/BR 10 2014 021620-0/RPI2589_180820\n",
    "# and this pattern to files\n",
    "#     PI 1104516-7_Despacho 7.1_180820_A.pdf\n",
    "patentes_df['f_name'] = patentes_df['registro'] + \"_Despacho \" + patentes_df['cod_despacho'] + \"_\" + patentes_df['data de publicação'].str[0:2] + patentes_df['data de publicação'].str[3:5] + patentes_df['data de publicação'].str[8:11]\n",
    "patentes_df['folder_name'] = \"PareceresRPI/\" + patentes_df['registro'] + \"/RPI\" + patentes_df['rpi'].astype(str) + \"_\" + patentes_df['data de publicação'].str[0:2] + patentes_df['data de publicação'].str[3:5] + patentes_df['data de publicação'].str[8:11]\n",
    "patentes_df.drop(patentes_df.columns.difference(['rpi','registro','cod_despacho','inpi_name','f_name','folder_name','cod_busca']), axis=1, inplace=True)\n",
    "\n",
    "# set te columns and create the dataframe to store information from INPI web server\n",
    "# for all selected editions\n",
    "column_names = ['inpi_name','rpi','cod_busca']\n",
    "df_inpi = pd.DataFrame(columns=column_names)\n",
    "\n",
    "rpi = rpi_i # set rpi counter as rpi_i\n",
    "for i in range(rpi_f-rpi_i+1): # for the i-th edition from the rpi\n",
    "    url = \"https://parecer.inpi.gov.br/arquivos/RPI/\" + str(rpi) # set the url to make the search\n",
    "    \n",
    "    if requests.get(url, verify=False).status_code != 200: # check if the files are avaiabel at web server\n",
    "        print(\"There are no files on the server for this edition.\")\n",
    "        sys.exit()\n",
    "    \n",
    "    # create a temporary dataframe to store the extract information from i-th edition\n",
    "    df_temp = pd.read_html(requests.get(url, verify=False).content, encoding='utf-8', header=0)[0].iloc[2:-1]\n",
    "    \n",
    "    df_temp.drop(df_temp.columns.difference(['Name']), axis=1, inplace=True) # drop all columns, except Name\n",
    "    df_temp.columns = ['inpi_name'] # rename the column\n",
    "    df_temp.loc[:,'rpi'] = rpi # create the columns with i-th RPI number for all records\n",
    "    df_temp['cod_busca'] = df_temp['inpi_name'].str[3:-11] # create the column with the search code\n",
    "    df_inpi = df_inpi.append(df_temp) # append the records for the i-th rpi at the end of INPI dataframe \n",
    "    del df_temp # delete temporary dataframe\n",
    "    rpi+=1 # ident the rpi number\n",
    "    \n",
    "df_inpi = df_inpi.reset_index(drop=True) # reset the index from rpi dataframe\n",
    "\n",
    "# merge the datframes with the information extracted from RPI and INPI web server\n",
    "patentes_df = pd.merge(patentes_df, df_inpi, left_on=['rpi','cod_busca'],right_on=['rpi','cod_busca'],how='left')    \n",
    "patentes_df.drop(['cod_busca'], axis=1, inplace=True) # after this, drop the column with the search code\n",
    "\n",
    "# remove the records that are not in both dataframes\n",
    "patentes_df.dropna(subset = ['registro'], inplace=True) # remove the lines where pi is NaN\n",
    "patentes_df.dropna(subset = ['inpi_name'], inplace=True) # remove the lines where inpi_name is NaN\n",
    "\n",
    "patentes_df['download'] = True # create a column and set dowload as True\n",
    "\n",
    "# cases where a protection has two orders in the same magazine\n",
    "# it is necessary to keep only the documents for one of the dispatch numbers\n",
    "# so set dowload as False\n",
    "patentes_df.loc[patentes_df.cod_despacho == '15.11', 'download'] = False # the documents are duplicated between 15.11 and 6.22\n",
    "patentes_df.loc[patentes_df.cod_despacho == '8.7', 'download'] = False # the documents are duplicated between 7.5 and 8.7\n",
    "# the patent certificate is not on the same server as the patent reports\n",
    "# and must be downloaded directly from the INPI website\n",
    "patentes_df.loc[patentes_df.cod_despacho == '16.1', 'download'] = False # set download as False\n",
    "patentes_df.loc[patentes_df['download'] == False,'download'] = np.nan # change False to NaN at download\n",
    "patentes_df.dropna(subset = ['download'], inplace=True) # remove the lines where download is NaN  \n",
    "patentes_df.drop(['download'], axis=1, inplace=True) # remove the download column\n",
    "\n",
    "# adding count flags to reports when there is more than one document\n",
    "patentes_df['dupl'] = patentes_df['registro'].duplicated(keep=False) # create dupl column\n",
    "patentes_df['flag'] = patentes_df.groupby('registro').cumcount() # group by register number and count\n",
    "patentes_df.loc[patentes_df['dupl'] == False, 'flag'] = \"\" # if there only one document, the flag column is empty\n",
    "# in this example, we changed the numeric counters to alphabetic counters using a dict\n",
    "dict_letters = {'' : '', 0 : '_A',1 : '_B', 2 : '_C', 3 : '_D', 4 : '_E', 5 : '_F', 6 : '_G', 7 : '_H', 8 : '_I', 9 : '_J'}\n",
    "patentes_df['flag']= patentes_df['flag'].map(dict_letters)  # change the counters\n",
    "patentes_df.drop(['dupl'], axis=1, inplace=True) # remove the column dupl\n",
    "patentes_df['f_name'] = patentes_df['f_name'] + patentes_df['flag'] + '.pdf' # add the count flags and pdf extension to filename\n",
    "patentes_df.drop(['flag'], axis=1, inplace=True) # remove the column flag\n",
    "\n",
    "i = 1 # start position in the dataframe to perform the download of the files\n",
    "# for the i-th elementh in the dataframe\n",
    "for folder_name,inpi_name,f_name,rpi in zip(patentes_df['folder_name'],patentes_df['inpi_name'],patentes_df['f_name'],patentes_df['rpi']):\n",
    "    if not os.path.exists(folder_name): # if the download folder not exist\n",
    "        os.makedirs(folder_name) # create the folder\n",
    "    url_cam = \"http://parecer.inpi.gov.br/download.php?cam=arquivos/RPI/\" + str(rpi) + \"/\" + inpi_name # set the url to download\n",
    "    f_request = requests.get(url_cam, verify = False)  # request data from the resource\n",
    "    path = folder_name + \"/\" + f_name # set the path to donwload the file\n",
    "    f = open(path, 'wb').write(f_request.content) # make the download\n",
    "    i+=1 # ident the position counter\n",
    "\n",
    "print(\"Download completed!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "pyINPI - Patentes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
